## Evaluation

**NOV 2025: In-Progress Draft; Requires exposition and citations**

__Plausibility, Refutation, Scope, and Sensitivity__

Use analytical tools from causal inference and qualitative research design:

*Plausibility*

- Do the results align with known cases or examples?
- Are the effect sizes reasonable, given the theoretical priors?
- Do the patterns hold across different sources of trace?

*Scope and Sensitivity*

- What are the limits of what can be learned from the designs permitted by the combination of trace availability, measurement, and estimation?  
- Where does the trace-measurement-estimation fall short?
- What unobserved pieces could change the results?

*Refutation*

-  Identify counterfactuals and alternative explanations. Evaluate whether the results of the research design are more/less consistent with alternative explanations than with the theoretical story that you are testing.

Draws from causal inference diagnostics, such as Cinelli & Hazlett, Imbens; evaluation frameworks for qualitative analysis; falsification tests (Rosenbaum)]

__Identification Evaluation__

Use theoretical priors as the scaffolding and structure to evaluate the results of the measurement and estimation. 

Treat theoretical expectations as an anchor box, and use multiple expectations to construct the framework and context, triangulating multiple signals.
