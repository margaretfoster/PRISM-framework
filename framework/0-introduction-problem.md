## PRISM: Introduction and Problem

Many high-stakes domains, from national security to model interpretation, are driven by mechanisms that are structurally unobservable. Further complicating analysis, data in these domains are often emergent, strategically obscured, or otherwise hostile to systematic model building and analysis.  

Researchers and practitioners operating in hostile data environments—militant group operations, sanctions evasion, informal economies, clandestine state programs—face a methodological catch-22: most causal identification and research design frameworks require data availability. 
PRISM bridges the gap between mainstream research design processes and opaque data environments by providing a structured framework for research design, measurement, and inference under hostile data conditions, such as adversarial data-generating processes, latent causal structures, and partial observability.

The central contribution of PRISM lies in the proposed structured design process and in proposing a trace→ measurement→ inference workflow. 
Applicable across domains, the framework is designed to align data-poor conditions, theoretical structure, and quantitative research tools into a design package that is defensible to rigorous methodological audiences.

PRISM draws on several intellectual traditions that develop pathways for causal and inferential reasoning under data constraints. 
The framework draws on the intellectual scaffolding developed for causal inference (Pearl, Green); process tracing (Bennett and Checkel 2012, 2015, Ricks and Liu 2015); and multi-method social science (Seawright 2016).  

As a workflow, PRISM is templated most closely on Sharma et al's Causal ML workflow.
PRISM is intended to structure the design of researchers looking to synthesize theory-forward research design, computational social science or AI/ML tools, and statistical identification and estimation methods. The approach is complementary to existing research design frameworks in substantive domains that are resistant to the precision and richness that these traditions often assume.

As an operationalization of human-in-the-loop inference, it connects particularly well with machine learning methods for modeling proxy and latent variables.
